import pdfplumber
import re
import pandas as pd
import os

# Paths
folder_path = r"***Put file here***"
master_csv_path = r"***Put file here***"

# Known body parts
body_parts = [
    "Left Arm", "Right Arm", "Left Leg", "Right Leg", "Left Ribs", "Right Ribs",
    "T Spine", "L Spine", "Pelvis", "SubTotal", "Head", "Total", "Android", "Gynoid"
]

def merge_body_part_names(row):
    """ Merge first two elements if they form a known body part name. """
    if len(row) > 1:
        potential_body_part = f"{row[0]} {row[1]}"
        if potential_body_part in body_parts:
            row[0:2] = [potential_body_part]  # Merge the first two elements
    return row

def parse_dexa_text(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()

            # Extract scan date for unique ID creation
            scan_date_match = re.search(r"Scan Date :([\d/]+)", text)
            scan_date = scan_date_match.group(1) if scan_date_match else "01/01/2000"  # Default date if missing

            # Convert scan date to consistent format
            try:
                scan_date = pd.to_datetime(scan_date, format="%d/%m/%Y").strftime("%d-%m-%Y")
            except ValueError:
                scan_date = "unknown_date"

            # Extract patient name
            patient_match = re.search(r"Patient :(.*?)\s+Height", text)
            patient_name = patient_match.group(1).strip().replace(" ", "_") if patient_match else "unknown_patient"

            # Create Unique ID for the scan
            unique_id = f"{patient_name}_{scan_date}"

            # Extract regional body part data
            body_parts_pattern = r"Left Arm.*Gynoid.*"
            body_parts_match = re.search(body_parts_pattern, text, re.DOTALL)

            data_rows = []
            if body_parts_match:
                body_parts_data = body_parts_match.group(0).split("\n")
                for line in body_parts_data:
                    if not re.search(r"\d", line):
                        continue

                    split_line = re.split(r"\s+", line.strip())
                    split_line = merge_body_part_names(split_line)

                    if len(split_line) == 9:  # Expected number of columns
                        body_part, *values = split_line
                        data_rows.append([unique_id, patient_name, scan_date, body_part] + values)

            return data_rows

def update_master_csv(folder_path, master_csv_path):
    headers = ["Unique ID", "Patient Name", "Scan Date", "Body Part", "% Fat", "Tissues (g)", "Tissue Area (cm²)", "Fat (g)", "Lean (g)", "BMC (g)", "BMC Area (cm²)", "Total Mass (kg)"]

    # Collect data from all PDF files in the folder
    all_data_rows = []
    pdf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(".pdf")]

    print(f"Found {len(pdf_files)} PDF files in folder: {folder_path}")
    for pdf_file in pdf_files:
        print(f"Processing file: {pdf_file}")
        data_rows = parse_dexa_text(pdf_file)
        if data_rows:
            all_data_rows.extend(data_rows)

    # Create DataFrame from parsed data
    new_data = pd.DataFrame(all_data_rows, columns=headers)

    # Ensure consistent date formatting
    new_data["Scan Date"] = pd.to_datetime(new_data["Scan Date"], format="%d-%m-%Y", errors="coerce")
    new_data["Scan Date"] = new_data["Scan Date"].dt.strftime("%d-%m-%Y")

    # Check if the master CSV exists
    if os.path.exists(master_csv_path):
        master_df = pd.read_csv(master_csv_path)
        master_df["Scan Date"] = pd.to_datetime(master_df["Scan Date"], format="%d-%m-%Y", errors="coerce")
        master_df["Scan Date"] = master_df["Scan Date"].dt.strftime("%d-%m-%Y")
    else:
        master_df = pd.DataFrame(columns=headers)

    # Append new data to master
    updated_df = pd.concat([master_df, new_data]).drop_duplicates(subset=["Unique ID", "Body Part"], keep="last")

    # Save the updated master CSV
    updated_df.to_csv(master_csv_path, index=False)
    print(f"Master CSV updated successfully! Total records: {len(updated_df)}")

# Run the batch update
update_master_csv(folder_path, master_csv_path)
